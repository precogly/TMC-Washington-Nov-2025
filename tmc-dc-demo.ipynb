{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53d357e2-e102-49e1-ab28-d35e69365d25",
   "metadata": {},
   "source": [
    "# A Generative Approach to Modeling Emergent Threats in Multi Agentic Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09d5fb8e-8364-4009-b5ad-b96de474d499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Demo Config ===\n",
      "Approach:             A2  (A2=full, A1=fallback)\n",
      "Use cached outputs:   True\n",
      "Live LLM calls:       False\n",
      "Primary model:        gpt-4o-mini\n",
      "Evaluation model:     gpt-4o\n",
      "Temperatures:         {'temporal_vuln': 0.3, 'interaction_vuln': 0.3, 'behavioral_vuln': 0.3, 'scamper_threats': 0.6, 'evaluation': 0.2}\n",
      "Seed:                 42\n",
      "Cache dir:            /Users/vikramnarayan/Documents/ERIMAS/TMC-Washington-Nov-2025/.cache\n",
      "Artifacts dir:        /Users/vikramnarayan/Documents/ERIMAS/TMC-Washington-Nov-2025/artifacts\n"
     ]
    }
   ],
   "source": [
    "# === Cell 2: Config & Runtime Switches =======================================\n",
    "# Demo mode controls for Approach 2 (with a fallback to Approach 1)\n",
    "# Notes:\n",
    "# - LLM = Large Language Model (expanded once per your preference)\n",
    "# - We’re using only GPT models for now.\n",
    "# - Temperature is phase-aware: more creative for ideation, lower for evaluation.\n",
    "\n",
    "from pathlib import Path\n",
    "import os, random\n",
    "import numpy as np\n",
    "\n",
    "# ---- Approach toggle ---------------------------------------------------------\n",
    "APPROACH = \"A2\"        # \"A2\" = full (graph + drift), \"A1\" = minimalist fallback\n",
    "\n",
    "# ---- Execution toggles -------------------------------------------------------\n",
    "USE_CACHED_OUTPUTS = True   # Load/save agent outputs to disk (good for offline demos)\n",
    "LIVE_LLM = False            # If False, only uses cached outputs (safe on bad Wi-Fi)\n",
    "\n",
    "# ---- Models (GPT only) -------------------------------------------------------\n",
    "# Keep them both GPT-family for now; you can switch to a larger judge later if needed.\n",
    "PRIMARY_MODEL = \"gpt-4o-mini\"   # used for vulnerability agents + SCAMPER ideation\n",
    "EVAL_MODEL    = \"gpt-4o\"        # used for evaluation/consensus (still GPT)\n",
    "\n",
    "# ---- Temperatures by phase ---------------------------------------------------\n",
    "# Lower = more deterministic; Higher = more creative.\n",
    "TEMPS = {\n",
    "    \"temporal_vuln\": 0.3,\n",
    "    \"interaction_vuln\": 0.3,\n",
    "    \"behavioral_vuln\": 0.3,\n",
    "    \"scamper_threats\": 0.6,   # a bit more creativity for story-like threats\n",
    "    \"evaluation\": 0.2,        # stricter scoring / less variance\n",
    "}\n",
    "\n",
    "def temp_for(phase: str) -> float:\n",
    "    return TEMPS.get(phase, 0.3)\n",
    "\n",
    "# ---- Reproducibility ---------------------------------------------------------\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "\n",
    "# ---- Paths (cache + outputs) -------------------------------------------------\n",
    "BASE_DIR = Path().resolve()  # current notebook folder\n",
    "CACHE_DIR = BASE_DIR / \".cache\"\n",
    "OUT_DIR   = BASE_DIR / \"artifacts\"\n",
    "CACHE_DIR.mkdir(exist_ok=True)\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# ---- Helper: show current config --------------------------------------------\n",
    "def print_config():\n",
    "    print(\"=== Demo Config ===\")\n",
    "    print(f\"Approach:             {APPROACH}  (A2=full, A1=fallback)\")\n",
    "    print(f\"Use cached outputs:   {USE_CACHED_OUTPUTS}\")\n",
    "    print(f\"Live LLM calls:       {LIVE_LLM}\")\n",
    "    print(f\"Primary model:        {PRIMARY_MODEL}\")\n",
    "    print(f\"Evaluation model:     {EVAL_MODEL}\")\n",
    "    print(f\"Temperatures:         {TEMPS}\")\n",
    "    print(f\"Seed:                 {SEED}\")\n",
    "    print(f\"Cache dir:            {CACHE_DIR}\")\n",
    "    print(f\"Artifacts dir:        {OUT_DIR}\")\n",
    "\n",
    "print_config()\n",
    "# ============================================================================ #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9bf2763-176a-4ec9-bbcf-e3532f5b6b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ OpenAI client initialized.\n",
      "Key prefix: sk-proj-... (hidden)\n"
     ]
    }
   ],
   "source": [
    "# === Cell 2.1: Load Environment Variables and Initialize OpenAI ===============\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "import os\n",
    "\n",
    "# Load variables from .env file in the project root\n",
    "load_dotenv()\n",
    "\n",
    "# Check for key\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise EnvironmentError(\"❌ OPENAI_API_KEY not found. Please create a .env file in the project root.\")\n",
    "\n",
    "# Initialize client\n",
    "openai.api_key = api_key\n",
    "\n",
    "# (Optional) small confirmation print, without leaking the key\n",
    "print(\"✅ OpenAI client initialized.\")\n",
    "print(f\"Key prefix: {api_key[:8]}... (hidden)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d35e19-0f00-4dab-8aa3-5a1f06cd4dcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
